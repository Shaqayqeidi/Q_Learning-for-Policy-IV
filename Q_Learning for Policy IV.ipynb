{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q_Learning for Policy IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset1():\n",
    "    st= [0]*16\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = [[0,2],[2,7],[2,4],[6,5],[3,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2], [2, 7], [2, 4], [6, 5], [3, 1]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 in fi[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def member(i , fi):\n",
    "    mem=[]\n",
    "    if i in fi[0]:\n",
    "        if i !=fi[0][0]:\n",
    "            mem.append(fi[0][0])\n",
    "        if i !=fi[0][1]:\n",
    "            mem.append(fi[0][1])\n",
    "        \n",
    "    if i in fi[1]:\n",
    "        if i !=fi[1][0]:\n",
    "            mem.append(fi[1][0])\n",
    "        if i !=fi[1][1]:\n",
    "            mem.append(fi[1][1])\n",
    "        \n",
    "    if i in fi[2]:\n",
    "        if i !=fi[2][0]:\n",
    "            mem.append(fi[2][0])\n",
    "        if i !=fi[2][1]:\n",
    "            mem.append(fi[2][1])\n",
    "    \n",
    "    if i in fi[3]:\n",
    "        if i !=fi[3][0]:\n",
    "            mem.append(fi[3][0])\n",
    "        if i !=fi[3][1]:\n",
    "            mem.append(fi[3][1])\n",
    "    \n",
    "    if i in fi[4]:\n",
    "        if i !=fi[4][0]:\n",
    "            mem.append(fi[4][0])\n",
    "        if i !=fi[4][1]:\n",
    "            mem.append(fi[4][1])\n",
    "    \n",
    "    return mem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "member(1,fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857\n"
     ]
    }
   ],
   "source": [
    "tpi =0\n",
    "for i in range(len(member(1,fi))):\n",
    "    tpi+=tp[member(1,fi)[i]] \n",
    "print(tpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.277"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((tpi,tp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(member(2,fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step (action ,st , i ):\n",
    "    f = random.weibullvariate(weibull_scale[i],weibull_shape[i])\n",
    "    \n",
    "    if action == 0 :\n",
    "        st[i] +=5 #age \n",
    "        \n",
    "        if f <= st[i]:\n",
    "            st[i+8]=1    \n",
    "        else:\n",
    "            st[i+8]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[i]=0\n",
    "            st[i+8]=0\n",
    "            \n",
    "            for j in range(len(member(i,fi))):\n",
    "                st[member(i,fi)[j]] = 0 \n",
    "                st[member(i,fi)[j]+8] = 0 \n",
    "            \n",
    "           \n",
    "    return tuple(st)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5, 0, 5, 8, 41, 2, 8, 0, 42, 0, 25, 14, 35, 12, 24, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(1,list((500,5,2,5,8,41,2,8,85,42,1,25,14,35,12,24,0)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun (action,st,i):\n",
    "    reward =[]\n",
    "    \n",
    "    if action == 1 :\n",
    "        tpi = 0 \n",
    "        for j in range(len(member(i,fi))):\n",
    "            tpi+=tp[member(i,fi)[j]]\n",
    "        \n",
    "        reward = - (time_interval/(0.8*time_interval*sum((tpi,tp[i]))))*0.8*sum((tpi,tp[i]))\n",
    "    \n",
    "    if (st[i+8]==1 and action == 0):\n",
    "        tpi = 0 \n",
    "        for j in range(len(member(i,fi))):\n",
    "            tpi+=tp[member(i,fi)[j]]\n",
    "            \n",
    "        tfi = 0\n",
    "        for j in range(len(member(i,fi))):\n",
    "            tfi+=tf[member(i,fi)[j]]\n",
    "            \n",
    "        reward = - (time_interval/(0.8*time_interval*sum((tpi,tp[i]))))*time_interval*math.ceil(0.8*sum((tfi,tf[i]))/time_interval)\n",
    "            \n",
    "    if (st[i+8]==0 and action == 0):\n",
    "        \n",
    "        reward = 5 \n",
    "        \n",
    "    return reward\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardfun(0,(500,5,2,5,8,41,2,8,0,42,1,25,14,35,12,24,0),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(epsilon, state,i):\n",
    "    if (state[1]==1): # we had failur in component i\n",
    "        return 1\n",
    "    else:\n",
    "        if (np.random.random() < epsilon):\n",
    "            return random.choice([0,1]) \n",
    "        else:\n",
    "            return np.argmax(Q_table[state])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "state = (500,5,2,5,8,41,2,8,0,42,10,25,14,35,12,24,0)\n",
    "state1 = [state[2],state[10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_action(0.5, state1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=1000\n",
    "min_lr=0.1 \n",
    "min_epsilon=0.1\n",
    "discount=0.9\n",
    "decay=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate( t):\n",
    "    \n",
    "    \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
    "    # Learning rate also declines as we add more episodes\n",
    "    return max(min_lr, min(1., 1. - math.log10((t + 1) / decay)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "i=0\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "                    \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        current_statey = (current_state[i],current_state[i+8])\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_statey , i)\n",
    "        # Take action\n",
    "        obs = step(action , list(current_state),i)\n",
    "        reward = rewardfun(action,current_state,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        new_statey = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statey][action] += (learning_rate * \n",
    "                                        (reward + discount * np.max(Q_table[new_statey])\n",
    "                                         - Q_table[current_statey][action]))\n",
    "                                         \n",
    "                                         \n",
    "        current_state = new_state \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the policy performance\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "y = scores\n",
    "plt.scatter(x, y, marker='x', c=y)\n",
    "fit = np.polyfit(x, y, deg=4)\n",
    "p = np.poly1d(fit) \n",
    "plt.plot(x,p(x),\"r--\") \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title(' performance ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "i=0\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs = step(action,list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "i=1\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "                    \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        current_statey = (current_state[i],current_state[i+8])\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_statey , i)\n",
    "        # Take action\n",
    "        obs = step(action , list(current_state),i)\n",
    "        reward = rewardfun(action,current_state,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        new_statey = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statey][action] += (learning_rate * \n",
    "                                        (reward + discount * np.max(Q_table[new_statey])\n",
    "                                         - Q_table[current_statey][action]))\n",
    "                                         \n",
    "                                         \n",
    "        current_state = new_state \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "i=1\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs = step(action,list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wheel rim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "i=2\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "                    \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        current_statey = (current_state[i],current_state[i+8])\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_statey , i)\n",
    "        # Take action\n",
    "        obs = step(action , list(current_state),i)\n",
    "        reward = rewardfun(action,current_state,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        new_statey = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statey][action] += (learning_rate * \n",
    "                                        (reward + discount * np.max(Q_table[new_statey])\n",
    "                                         - Q_table[current_statey][action]))\n",
    "                                         \n",
    "                                         \n",
    "        current_state = new_state \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "i=2\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs = step(action,list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "i=3\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "                    \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        current_statey = (current_state[i],current_state[i+8])\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_statey , i)\n",
    "        # Take action\n",
    "        obs = step(action , list(current_state),i)\n",
    "        reward = rewardfun(action,current_state,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        new_statey = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statey][action] += (learning_rate * \n",
    "                                        (reward + discount * np.max(Q_table[new_statey])\n",
    "                                         - Q_table[current_statey][action]))\n",
    "                                         \n",
    "                                         \n",
    "        current_state = new_state \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "i=3\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs = step(action,list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "i=4\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "                    \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        current_statey = (current_state[i],current_state[i+8])\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_statey , i)\n",
    "        # Take action\n",
    "        obs = step(action , list(current_state),i)\n",
    "        reward = rewardfun(action,current_state,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        new_statey = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statey][action] += (learning_rate * \n",
    "                                        (reward + discount * np.max(Q_table[new_statey])\n",
    "                                         - Q_table[current_statey][action]))\n",
    "                                         \n",
    "                                         \n",
    "        current_state = new_state \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "i=4\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs = step(action,list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "i=5\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "                    \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        current_statey = (current_state[i],current_state[i+8])\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_statey , i)\n",
    "        # Take action\n",
    "        obs = step(action , list(current_state),i)\n",
    "        reward = rewardfun(action,current_state,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        new_statey = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statey][action] += (learning_rate * \n",
    "                                        (reward + discount * np.max(Q_table[new_statey])\n",
    "                                         - Q_table[current_statey][action]))\n",
    "                                         \n",
    "                                         \n",
    "        current_state = new_state \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "i=5\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs = step(action,list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steering Wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "i=6\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "                    \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        current_statey = (current_state[i],current_state[i+8])\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_statey , i)\n",
    "        # Take action\n",
    "        obs = step(action , list(current_state),i)\n",
    "        reward = rewardfun(action,current_state,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        new_statey = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statey][action] += (learning_rate * \n",
    "                                        (reward + discount * np.max(Q_table[new_statey])\n",
    "                                         - Q_table[current_statey][action]))\n",
    "                                         \n",
    "                                         \n",
    "        current_state = new_state \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "i=6\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs = step(action,list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting Gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "i=7\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "                    \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        current_statey = (current_state[i],current_state[i+8])\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_statey , i)\n",
    "        # Take action\n",
    "        obs = step(action , list(current_state),i)\n",
    "        reward = rewardfun(action,current_state,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        new_statey = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statey][action] += (learning_rate * \n",
    "                                        (reward + discount * np.max(Q_table[new_statey])\n",
    "                                         - Q_table[current_statey][action]))\n",
    "                                         \n",
    "                                         \n",
    "        current_state = new_state \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1105"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "i=7\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs = step(action,list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
